{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tdt-xed-separate-labels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OXkxjlRB_Dq"
      },
      "source": [
        "Combine training samples from TDT and XED, but keep their labels separate. This does not seem to benefit either data set in evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6eK9-OgCBFe"
      },
      "source": [
        "# Set the file paths here\r\n",
        "tdt_train_fn = '/content/tdt-sentiment-151020-train-clean.tsv'\r\n",
        "tdt_eval_fn = '/content/tdt-sentiment-151020-dev.tsv'\r\n",
        "xed_nonneutrals_fn = '/content/fi-annotated.tsv'\r\n",
        "xed_neutrals_fn = '/content/neu_fi.txt'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlE0HrlD-6J8"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmWefADy_G1S"
      },
      "source": [
        "# Choose model and set up input\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizerFast\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "def transpose(l):\n",
        "  return [list(t) for t in zip(*l)]\n",
        "\n",
        "def load_fields(fn):\n",
        "  return transpose([l.rstrip('\\n').split('\\t') for l in open(fn).readlines()])\n",
        "\n",
        "def label_vector(num_labels, labels):\n",
        "  binary_labels = []\n",
        "  for ls in labels:\n",
        "    b = [0]*num_labels\n",
        "    for l in ls:\n",
        "      b[l] = 1\n",
        "    binary_labels.append(b)\n",
        "  return binary_labels\n",
        "\n",
        "tdt_train_texts, tdt_train_labels_raw = load_fields(tdt_train_fn)[1:3]\n",
        "tdt_label_dictionary = {'positive': 9, 'negative': 10, 'other': 11, 'neutral': 12}\n",
        "tdt_train_labels = [[tdt_label_dictionary[l]] for l in tdt_train_labels_raw]\n",
        "\n",
        "tdt_eval_texts, tdt_eval_labels_raw = load_fields(tdt_eval_fn)[1:3]\n",
        "tdt_eval_labels = [[tdt_label_dictionary[l]] for l in tdt_eval_labels_raw]\n",
        "\n",
        "neutral_texts = load_fields(xed_neutrals_fn)[1]\n",
        "neutral_labels = [[8]]*len(neutral_texts)\n",
        "\n",
        "nonneutral_texts, labels_raw = load_fields(xed_nonneutrals_fn)[:2]\n",
        "nonneutral_labels = [[int(s) for s in l.replace('8', '0').split(',')] for l in labels_raw]\n",
        "\n",
        "xed_texts = neutral_texts + nonneutral_texts\n",
        "xed_labels = neutral_labels + nonneutral_labels\n",
        "\n",
        "num_labels = 13\n",
        "\n",
        "tdt_train_label_vectors = label_vector(num_labels, tdt_train_labels)\n",
        "tdt_eval_label_vectors = label_vector(num_labels, tdt_eval_labels)\n",
        "xed_label_vectors = label_vector(num_labels, xed_labels)\n",
        "\n",
        "xed_train_texts, xed_eval_texts, xed_train_label_vectors, xed_eval_label_vectors = train_test_split(xed_texts, xed_label_vectors, test_size=0.1)\n",
        "\n",
        "train_texts = tdt_train_texts + xed_train_texts\n",
        "train_label_vectors = tf.constant(tdt_train_label_vectors + xed_train_label_vectors, dtype='float32')\n",
        "\n",
        "eval_texts = tdt_eval_texts + xed_eval_texts\n",
        "eval_label_vectors = tf.constant(tdt_eval_label_vectors + xed_eval_label_vectors, dtype='float32')\n",
        "\n",
        "#model_name = \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
        "model_name = \"TurkuNLP/bert-base-finnish-uncased-v1\"\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "input_size = 128\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding='longest', max_length=input_size)\n",
        "eval_encodings = tokenizer(eval_texts, truncation=True, padding='longest', max_length=input_size)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gzBHqOVVn66"
      },
      "source": [
        "# Set up training\n",
        "from transformers import TFBertForSequenceClassification, optimization_tf\n",
        "\n",
        "init_lr = 2e-5\n",
        "\n",
        "epochs = 2\n",
        "batch_size_train = 16\n",
        "batch_size_eval = 16\n",
        "\n",
        "def train(model, t, train_labels, eval):\n",
        "  size_train = len(train_labels)\n",
        "  steps_per_epoch = int(size_train/batch_size_train)\n",
        "  steps_train = steps_per_epoch*epochs\n",
        "  steps_warmup = int(epochs * size_train * 0.1 / batch_size_train)\n",
        "  optimizer, _ = optimization_tf.create_optimizer(init_lr=init_lr,\n",
        "                                                  num_train_steps=steps_train,\n",
        "                                                  num_warmup_steps=steps_warmup,\n",
        "                                                  weight_decay_rate=0.01)\n",
        "  model.compile(optimizer=optimizer, loss=tf.nn.sigmoid_cross_entropy_with_logits, metrics=[])\n",
        "  model.fit(t,\n",
        "            train_labels,\n",
        "            validation_data=eval,\n",
        "            batch_size=batch_size_train,\n",
        "            epochs=epochs)\n",
        "  return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4aPEfjuY0IZ"
      },
      "source": [
        "# Evaluate\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def train_evaluate(train_x, train_y, eval_x, eval_y, num_labels, run_count):\n",
        "  runs = []\n",
        "  for i in range(run_count):\n",
        "    bert = TFBertForSequenceClassification.from_pretrained(model_name,\n",
        "                                                           num_labels=num_labels)\n",
        "    bert = train(bert, train_x, train_y, (eval_x, eval_y))\n",
        "    runs.append(bert.predict(eval_x)[0])\n",
        "  return runs\n",
        "\n",
        "def format_floats(l):\n",
        "  return ', '.join(f'{x:.4f}' for x in l)\n",
        "\n",
        "def print_results(train_x, train_y, eval_x, eval_y, num_labels, run_count):\n",
        "  runs = train_evaluate(train_x, train_y, eval_x, eval_y, num_labels, run_count)\n",
        "  preds = [(tf.math.sigmoid(r) >= 0.5).numpy().tolist() for r in runs]\n",
        "  print(f\"Model: {model_name}, initial learning rate = {init_lr}, input size = {input_size}, batch size = {batch_size_train}, epochs = {epochs}\")\n",
        "  unpredicted_formatted = [f\"{sum([1 if not 1 in v else 0 for v in p])} out of {len(p)}\" for p in preds]\n",
        "  print(f\"Number of sentences with no predicted labels: {unpredicted_formatted}\")\n",
        "  accuracy = [accuracy_score(eval_label_vectors, p) for p in preds]\n",
        "  weighted_f1 = [f1_score(eval_label_vectors, p, average='weighted') for p in preds]\n",
        "  print(f\"Accuracy: {format_floats(accuracy)}\")\n",
        "  print(f\"Weighted F-score: {format_floats(weighted_f1)}\")\n",
        "  print(f'Average accuracy: {np.mean(accuracy):.4f}, stdev: {np.std(accuracy):.4f}')\n",
        "  print(f'Average weighted F-score: {np.mean(weighted_f1):.4f}, stdev: {np.std(weighted_f1):.4f}')\n",
        "  max_i = accuracy.index(max(accuracy))\n",
        "  print(classification_report(eval_label_vectors, preds[max_i], target_names=['H:trust', 'H:anger', 'H:anticipation', 'H:disgust', 'H:fear', 'H:joy', 'H:sadness', 'H:surprise', 'H:neutral', 'T:positive', 'T:negative', 'T:other', 'T:neutral'], digits=4))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-2lraFFkMiK",
        "outputId": "4c71c1c1-8867-4ef3-fc9e-2b4656d8fdbf"
      },
      "source": [
        "# Evaluate with no source label in sentences\n",
        "\n",
        "t = [tf.constant(train_encodings.data['input_ids']),\n",
        "     tf.constant(train_encodings.data['attention_mask']),\n",
        "     tf.constant(train_encodings.data['token_type_ids'])]\n",
        "\n",
        "e = [tf.constant(eval_encodings.data['input_ids']),\n",
        "     tf.constant(eval_encodings.data['attention_mask']),\n",
        "     tf.constant(eval_encodings.data['token_type_ids'])]\n",
        "\n",
        "print_results(t, train_label_vectors, e, eval_label_vectors, num_labels, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 and are newly initialized: ['classifier', 'dropout_379']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some layers from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 and are newly initialized: ['dropout_417', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some layers from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 and are newly initialized: ['classifier', 'dropout_455']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: TurkuNLP/bert-base-finnish-uncased-v1, initial learning rate = 2e-05, input size = 128, batch size = 16, epochs = 1\n",
            "Number of sentences with no predicted labels: ['0 out of 3706', '0 out of 3706', '0 out of 3706']\n",
            "Accuracy: 0.0000, 0.0000, 0.0000\n",
            "Weighted F-score: 0.1755, 0.0819, 0.1409\n",
            "Average accuracy: 0.0000, stdev: 0.0000\n",
            "Average weighted F-score: 0.1327, stdev: 0.0387\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "       H:trust     0.0347    0.1129    0.0530       248\n",
            "       H:anger     0.0304    0.0579    0.0399       311\n",
            "H:anticipation     0.0586    0.8073    0.1094       218\n",
            "     H:disgust     0.0204    0.0047    0.0076       215\n",
            "        H:fear     0.0523    0.4054    0.0927       222\n",
            "         H:joy     0.0244    0.0084    0.0125       239\n",
            "     H:sadness     0.0560    0.9798    0.1060       198\n",
            "    H:surprise     0.0000    0.0000    0.0000       185\n",
            "     H:neutral     0.4003    0.2726    0.3244      1104\n",
            "    T:positive     0.0274    0.8962    0.0532       106\n",
            "    T:negative     0.0262    0.5312    0.0499       128\n",
            "       T:other     0.0164    0.6588    0.0319        85\n",
            "    T:negative     0.2214    0.4443    0.2955       862\n",
            "\n",
            "     micro avg     0.0649    0.3426    0.1091      4121\n",
            "     macro avg     0.0745    0.3984    0.0905      4121\n",
            "  weighted avg     0.1709    0.3426    0.1755      4121\n",
            "   samples avg     0.0641    0.3501    0.1064      4121\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLO86gEukhfF",
        "outputId": "f5d5dd11-750f-433e-9501-0cd720f9c26c"
      },
      "source": [
        "# Evaluate with source label in sentences\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "train_texts_source = ['T:' + t for t in tdt_train_texts] + ['H:' + t for t in xed_train_texts]\n",
        "eval_texts_source = ['T:' + t for t in tdt_eval_texts] + ['H:' + t for t in xed_eval_texts]\n",
        "\n",
        "tdt_eval_texts_source = ['T:' + t for t in tdt_eval_texts]\n",
        "xed_eval_texts_source = ['H:' + t for t in xed_eval_texts]\n",
        "\n",
        "train_encodings = tokenizer(train_texts_source, truncation=True, padding='longest', max_length=input_size)\n",
        "eval_encodings = tokenizer(eval_texts_source, truncation=True, padding='longest', max_length=input_size)\n",
        "\n",
        "tdt_eval_encodings = tokenizer(tdt_eval_texts_source, truncation=True, padding='longest', max_length=input_size)\n",
        "xed_eval_encodings = tokenizer(xed_eval_texts_source, truncation=True, padding='longest', max_length=input_size)\n",
        "\n",
        "t_source = [tf.constant(train_encodings.data['input_ids']),\n",
        "            tf.constant(train_encodings.data['attention_mask']),\n",
        "            tf.constant(train_encodings.data['token_type_ids'])]\n",
        "\n",
        "e_source = [tf.constant(eval_encodings.data['input_ids']),\n",
        "            tf.constant(eval_encodings.data['attention_mask']),\n",
        "            tf.constant(eval_encodings.data['token_type_ids'])]\n",
        "\n",
        "e_tdt = [tf.constant(tdt_eval_encodings.data['input_ids']),\n",
        "         tf.constant(tdt_eval_encodings.data['attention_mask']),\n",
        "         tf.constant(tdt_eval_encodings.data['token_type_ids'])]\n",
        "\n",
        "e_xed = [tf.constant(xed_eval_encodings.data['input_ids']),\n",
        "         tf.constant(xed_eval_encodings.data['attention_mask']),\n",
        "         tf.constant(xed_eval_encodings.data['token_type_ids'])]\n",
        "\n",
        "eval_xs = [e_tdt, e_xed, e_source]\n",
        "eval_ys = [tf.constant(tdt_eval_label_vectors, dtype='float32'),\n",
        "           tf.constant(xed_eval_label_vectors, dtype='float32'),\n",
        "           eval_label_vectors]\n",
        "\n",
        "runs = []\n",
        "for i in range(3):\n",
        "  bert = TFBertForSequenceClassification.from_pretrained(model_name,\n",
        "                                                         num_labels=num_labels)\n",
        "  bert = train(bert, t_source, train_label_vectors, (e_source, eval_label_vectors))\n",
        "  runs.append([bert.predict(x)[0] for x in eval_xs])\n",
        "\n",
        "runs = transpose(runs)\n",
        "\n",
        "def format_floats(l):\n",
        "  return ', '.join(f'{x:.4f}' for x in l)\n",
        "\n",
        "print(f\"Model: {model_name}, initial learning rate = {init_lr}, input size = {input_size}, batch size = {batch_size_train}, epochs = {epochs}\")\n",
        "for eval_y, result, name in zip(eval_ys, runs, ['TDT', 'XED', 'TDT+XED']):\n",
        "  print(f'Results for evaluation on {name}')\n",
        "  preds_raw = [(tf.math.sigmoid(r) >= 0.5).numpy().tolist() for r in result]\n",
        "  preds = [[p if 1 in p else [e == max(r) for e in r] for r, p in zip(run.tolist(), pred)] for run, pred in zip(result, preds_raw)]\n",
        "  unpredicted_formatted = [f\"{sum([1 if not 1 in v else 0 for v in p])} out of {len(p)}\" for p in preds]\n",
        "  accuracy = [accuracy_score(eval_y, p) for p in preds]\n",
        "  weighted_f1 = [f1_score(eval_y, p, average='weighted') for p in preds]\n",
        "  print(f\"Accuracy: {format_floats(accuracy)}\")\n",
        "  print(f\"Weighted F-score: {format_floats(weighted_f1)}\")\n",
        "  print(f'Average accuracy: {np.mean(accuracy):.4f}, stdev: {np.std(accuracy):.4f}')\n",
        "  print(f'Average weighted F-score: {np.mean(weighted_f1):.4f}, stdev: {np.std(weighted_f1):.4f}')\n",
        "  max_i = accuracy.index(max(accuracy))\n",
        "  print(classification_report(eval_y, preds[max_i], target_names=['H:trust', 'H:anger', 'H:anticipation', 'H:disgust', 'H:fear', 'H:joy', 'H:sadness', 'H:surprise', 'H:neutral', 'T:positive', 'T:negative', 'T:other', 'T:neutral'], digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 and are newly initialized: ['classifier', 'dropout_265']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2106/2106 [==============================] - 1011s 480ms/step - loss: 0.1970 - val_loss: 0.1458\n",
            "Epoch 2/2\n",
            "2106/2106 [==============================] - 1005s 477ms/step - loss: 0.1279 - val_loss: 0.1396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 and are newly initialized: ['classifier', 'dropout_303']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2106/2106 [==============================] - 1000s 475ms/step - loss: 0.1965 - val_loss: 0.1454\n",
            "Epoch 2/2\n",
            "2106/2106 [==============================] - 993s 471ms/step - loss: 0.1290 - val_loss: 0.1400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at TurkuNLP/bert-base-finnish-uncased-v1 and are newly initialized: ['classifier', 'dropout_341']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2106/2106 [==============================] - 1003s 476ms/step - loss: 0.1990 - val_loss: 0.1459\n",
            "Epoch 2/2\n",
            "2106/2106 [==============================] - 1015s 482ms/step - loss: 0.1293 - val_loss: 0.1401\n",
            "Model: TurkuNLP/bert-base-finnish-uncased-v1, initial learning rate = 2e-05, input size = 128, batch size = 16, epochs = 2\n",
            "Results for evaluation on TDT\n",
            "Accuracy: 0.8772, 0.8755, 0.8679\n",
            "Weighted F-score: 0.8745, 0.8703, 0.8648\n",
            "Average accuracy: 0.8736, stdev: 0.0041\n",
            "Average weighted F-score: 0.8699, stdev: 0.0040\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "       H:trust     0.0000    0.0000    0.0000         0\n",
            "       H:anger     0.0000    0.0000    0.0000         0\n",
            "H:anticipation     0.0000    0.0000    0.0000         0\n",
            "     H:disgust     0.0000    0.0000    0.0000         0\n",
            "        H:fear     0.0000    0.0000    0.0000         0\n",
            "         H:joy     0.0000    0.0000    0.0000         0\n",
            "     H:sadness     0.0000    0.0000    0.0000         0\n",
            "    H:surprise     0.0000    0.0000    0.0000         0\n",
            "     H:neutral     0.0000    0.0000    0.0000         0\n",
            "    T:positive     0.7982    0.8585    0.8273       106\n",
            "    T:negative     0.8021    0.6016    0.6875       128\n",
            "       T:other     0.7101    0.5765    0.6364        85\n",
            "     T:neutral     0.9085    0.9559    0.9316       862\n",
            "\n",
            "     micro avg     0.8777    0.8815    0.8796      1181\n",
            "     macro avg     0.2476    0.2302    0.2371      1181\n",
            "  weighted avg     0.8728    0.8815    0.8745      1181\n",
            "   samples avg     0.8793    0.8815    0.8800      1181\n",
            "\n",
            "Results for evaluation on XED\n",
            "Accuracy: 0.5192, 0.5299, 0.5192\n",
            "Weighted F-score: 0.5327, 0.5395, 0.5346\n",
            "Average accuracy: 0.5228, stdev: 0.0050\n",
            "Average weighted F-score: 0.5356, stdev: 0.0029\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "       H:trust     0.5198    0.4163    0.4623       221\n",
            "       H:anger     0.5238    0.3507    0.4201       345\n",
            "H:anticipation     0.5463    0.2479    0.3410       238\n",
            "     H:disgust     0.5419    0.4236    0.4755       229\n",
            "        H:fear     0.4653    0.3300    0.3862       203\n",
            "         H:joy     0.6332    0.5800    0.6054       250\n",
            "     H:sadness     0.5000    0.4465    0.4717       215\n",
            "    H:surprise     0.4940    0.1806    0.2645       227\n",
            "     H:neutral     0.6907    0.7940    0.7387      1063\n",
            "    T:positive     0.0000    0.0000    0.0000         0\n",
            "    T:negative     0.0000    0.0000    0.0000         0\n",
            "       T:other     0.0000    0.0000    0.0000         0\n",
            "     T:neutral     0.0000    0.0000    0.0000         0\n",
            "\n",
            "     micro avg     0.6090    0.5222    0.5623      2991\n",
            "     macro avg     0.3781    0.2900    0.3204      2991\n",
            "  weighted avg     0.5872    0.5222    0.5395      2991\n",
            "   samples avg     0.6083    0.5712    0.5809      2991\n",
            "\n",
            "Results for evaluation on TDT+XED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6333, 0.6400, 0.6303\n",
            "Weighted F-score: 0.6294, 0.6331, 0.6280\n",
            "Average accuracy: 0.6346, stdev: 0.0041\n",
            "Average weighted F-score: 0.6302, stdev: 0.0022\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "       H:trust     0.5198    0.4163    0.4623       221\n",
            "       H:anger     0.5238    0.3507    0.4201       345\n",
            "H:anticipation     0.5463    0.2479    0.3410       238\n",
            "     H:disgust     0.5419    0.4236    0.4755       229\n",
            "        H:fear     0.4653    0.3300    0.3862       203\n",
            "         H:joy     0.6332    0.5800    0.6054       250\n",
            "     H:sadness     0.5000    0.4465    0.4717       215\n",
            "    H:surprise     0.4940    0.1806    0.2645       227\n",
            "     H:neutral     0.6907    0.7940    0.7387      1063\n",
            "    T:positive     0.7706    0.7925    0.7814       106\n",
            "    T:negative     0.7812    0.5859    0.6696       128\n",
            "       T:other     0.7937    0.5882    0.6757        85\n",
            "     T:neutral     0.9037    0.9582    0.9302       862\n",
            "\n",
            "     micro avg     0.6931    0.6225    0.6559      4172\n",
            "     macro avg     0.6280    0.5150    0.5556      4172\n",
            "  weighted avg     0.6674    0.6225    0.6331      4172\n",
            "   samples avg     0.6936    0.6684    0.6750      4172\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5lTCBHJBRit",
        "outputId": "4a5ea24b-0ad1-48af-bbba-81902a82ce88"
      },
      "source": [
        "result = runs[2]\n",
        "\n",
        "preds_raw = [(tf.math.sigmoid(r) >= 0.5).numpy().tolist() for r in result]\n",
        "preds = [[p if 1 in p else [e == max(r) for e in r] for r, p in zip(run.tolist(), pred)] for run, pred in zip(result, preds_raw)]\n",
        "indexes = random.sample(range(len(preds_raw[0])), 10)\n",
        "unpredicted_formatted = [f\"{sum([1 if not 1 in v else 0 for v in p])} out of {len(p)}\" for p in preds]\n",
        "accuracy = [accuracy_score(eval_y, p) for p in preds]\n",
        "weighted_f1 = [f1_score(eval_y, p, average='weighted') for p in preds]\n",
        "print(f\"Accuracy: {format_floats(accuracy)}\")\n",
        "print(f\"Weighted F-score: {format_floats(weighted_f1)}\")\n",
        "print(f'Average accuracy: {np.mean(accuracy):.4f}, stdev: {np.std(accuracy):.4f}')\n",
        "print(f'Average weighted F-score: {np.mean(weighted_f1):.4f}, stdev: {np.std(weighted_f1):.4f}')\n",
        "max_i = accuracy.index(max(accuracy))\n",
        "tdt_label_list = ['T:positive', 'T:negative', 'T:other', 'T:neutral']\n",
        "xed_label_list = ['H:trust', 'H:anger', 'H:anticipation', 'H:disgust', 'H:fear', 'H:joy', 'H:sadness', 'H:surprise', 'H:neutral']\n",
        "print(classification_report(eval_y, preds[max_i], target_names=xed_label_list + tdt_label_list, digits=4))\n",
        "tdt_micro_f1 = [f1_score(eval_y, p, labels=[9,10,11,12], average='micro') for p in preds]\n",
        "xed_micro_f1 = [f1_score(eval_y, p, labels=list(range(9)), average='micro') for p in preds]\n",
        "print(f'Micro-average F-score for TDT: {format_floats(tdt_micro_f1)}')\n",
        "print(f'Average: {np.mean(tdt_micro_f1):.4f}, stdev: {np.std(tdt_micro_f1):.4f}\\n')\n",
        "print(f'Micro-average F-score for XED: {format_floats(xed_micro_f1)}')\n",
        "print(f'Average: {np.mean(xed_micro_f1):.4f}, stdev: {np.std(xed_micro_f1):.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 3, 7], [1, 3, 4], [1], [1], [1, 4]]\n",
            "[3, 3, 1, 1, 2]\n",
            "14449\n",
            "3381\n",
            "Accuracy: 0.6333, 0.6400, 0.6303\n",
            "Weighted F-score: 0.6294, 0.6331, 0.6280\n",
            "Average accuracy: 0.6346, stdev: 0.0041\n",
            "Average weighted F-score: 0.6302, stdev: 0.0022\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "       H:trust     0.5198    0.4163    0.4623       221\n",
            "       H:anger     0.5238    0.3507    0.4201       345\n",
            "H:anticipation     0.5463    0.2479    0.3410       238\n",
            "     H:disgust     0.5419    0.4236    0.4755       229\n",
            "        H:fear     0.4653    0.3300    0.3862       203\n",
            "         H:joy     0.6332    0.5800    0.6054       250\n",
            "     H:sadness     0.5000    0.4465    0.4717       215\n",
            "    H:surprise     0.4940    0.1806    0.2645       227\n",
            "     H:neutral     0.6907    0.7940    0.7387      1063\n",
            "    T:positive     0.7706    0.7925    0.7814       106\n",
            "    T:negative     0.7812    0.5859    0.6696       128\n",
            "       T:other     0.7937    0.5882    0.6757        85\n",
            "     T:neutral     0.9037    0.9582    0.9302       862\n",
            "\n",
            "     micro avg     0.6931    0.6225    0.6559      4172\n",
            "     macro avg     0.6280    0.5150    0.5556      4172\n",
            "  weighted avg     0.6674    0.6225    0.6331      4172\n",
            "   samples avg     0.6936    0.6684    0.6750      4172\n",
            "\n",
            "Micro-average F-score for TDT: 0.8796, 0.8760, 0.8684\n",
            "Average: 0.8747, stdev: 0.0047\n",
            "\n",
            "Micro-average F-score for XED: 0.5534, 0.5623, 0.5538\n",
            "Average: 0.5565, stdev: 0.0041\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}