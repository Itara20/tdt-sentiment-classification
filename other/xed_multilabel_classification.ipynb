{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xed-multilabel-classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAwQHYNqfc5o"
      },
      "source": [
        "A straightforward multilabel classification experiment using XED nonneutrals as data and TFBertForSequenceClassification with FinBERT as the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qhbGHBX0KDP"
      },
      "source": [
        "# Set the file paths here\r\n",
        "xed_nonneutrals_fn = '/content/fi-annotated.tsv'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlN1YOnjgry_"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG5t_Gwrgw8g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa2cc344-a900-486a-ed0d-ff09a5ddf57f"
      },
      "source": [
        "# Choose model and set up input\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizerFast\n",
        "import tensorflow as tf\n",
        "\n",
        "def transpose(l):\n",
        "  return [list(t) for t in zip(*l)]\n",
        "\n",
        "def load_fields(fn):\n",
        "  return transpose([l.rstrip('\\n').split('\\t') for l in open(fn).readlines()])\n",
        "\n",
        "texts, labels_raw = load_fields(xed_nonneutrals_fn)\n",
        "\n",
        "num_labels = 8\n",
        "\n",
        "labels = [[int(s) for s in l.replace('8', '0').split(',')] for l in labels_raw]\n",
        "\n",
        "binary_labels = []\n",
        "for ls in labels:\n",
        "  b = [0]*num_labels\n",
        "  for l in ls:\n",
        "    b[l] = 1\n",
        "  binary_labels.append(b)\n",
        "\n",
        "print(texts[:5])\n",
        "print(binary_labels[:5])\n",
        "\n",
        "train_texts, eval_texts, train_labels, eval_labels = train_test_split(texts, binary_labels, test_size=0.1)\n",
        "\n",
        "model_name = \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
        "#model_name = \"TurkuNLP/bert-base-finnish-uncased-v1\"\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "input_size = 64\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding='longest', max_length=input_size)\n",
        "eval_encodings = tokenizer(eval_texts, truncation=True, padding='longest', max_length=input_size)\n",
        "\n",
        "t = [tf.constant(train_encodings.data['input_ids']),\n",
        "     tf.constant(train_encodings.data['attention_mask']),\n",
        "     tf.constant(train_encodings.data['token_type_ids'])]\n",
        "\n",
        "e = [tf.constant(eval_encodings.data['input_ids']),\n",
        "     tf.constant(eval_encodings.data['attention_mask']),\n",
        "     tf.constant(eval_encodings.data['token_type_ids'])]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Kuinka se vapautui niin nopeasti, mutta sinä et ole liikahtanutkaan?', 'Ruumiita ripustettuina - ruumiita, joiden nahka on nyljetty pois, sydämet revitty rinnoista.', 'Ei mitään muttia.', 'Älä anna hänen määräillä sinua!', 'Laske aseet maahan.']\n",
            "[[0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 1, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ay11VPmh8TT"
      },
      "source": [
        "# Set up training\r\n",
        "from transformers import TFBertForSequenceClassification, optimization_tf\r\n",
        "\r\n",
        "init_lr = 3e-5\r\n",
        "epochs = 2\r\n",
        "batch_size_train = 16\r\n",
        "\r\n",
        "def train(t, train_labels, eval, num_labels, init_lr, epochs, batch_size_train):\r\n",
        "  size_train = len(train_labels)\r\n",
        "  steps_per_epoch = int(size_train/batch_size_train)\r\n",
        "  steps_train = steps_per_epoch*epochs\r\n",
        "  steps_warmup = int(epochs * size_train * 0.1 / batch_size_train)\r\n",
        "\r\n",
        "  model = TFBertForSequenceClassification.from_pretrained(model_name,\r\n",
        "                                                          num_labels=num_labels)\r\n",
        "  optimizer, _ = optimization_tf.create_optimizer(init_lr=init_lr,\r\n",
        "                                                  num_train_steps=steps_train,\r\n",
        "                                                  num_warmup_steps=steps_warmup,\r\n",
        "                                                  weight_decay_rate=0.01)\r\n",
        "  model.compile(optimizer=optimizer,\r\n",
        "                loss=tf.nn.sigmoid_cross_entropy_with_logits,\r\n",
        "                metrics=[])\r\n",
        "  model.fit(t,\r\n",
        "            tf.constant(train_labels, dtype='float32'),\r\n",
        "            validation_data=(eval[0], tf.constant(eval[1], dtype='float32')),\r\n",
        "            batch_size=batch_size_train,\r\n",
        "            epochs=epochs)\r\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXEZSoSCijUz"
      },
      "source": [
        "# Set up evaluation\r\n",
        "\r\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def train_evaluate(runs, train_x, train_y, eval_x, eval_y, num_labels, init_lr, epochs, batch_size_train, run_count):\r\n",
        "  for i in range(run_count):\r\n",
        "    bert = train(train_x, train_y, (eval_x, eval_y), num_labels, init_lr, epochs, batch_size_train)\r\n",
        "    runs.append(bert.predict(eval_x)[0])\r\n",
        "\r\n",
        "def format_floats(l):\r\n",
        "  return ', '.join(f'{x:.4f}' for x in l)\r\n",
        "\r\n",
        "def print_results(runs, eval_labels):\r\n",
        "  preds = [(tf.math.sigmoid(r) >= 0.5).numpy().tolist() for r in runs]\r\n",
        "  accuracy = [accuracy_score(eval_labels, p) for p in preds]\r\n",
        "  weighted_f1 = [f1_score(eval_labels, p, average='weighted') for p in preds]\r\n",
        "  print(f\"Accuracy: {format_floats(accuracy)}\")\r\n",
        "  print(f\"Weighted F-score: {format_floats(weighted_f1)}\")\r\n",
        "  print(f'Average accuracy: {np.mean(accuracy):.4f}, stdev: {np.std(accuracy):.4f}')\r\n",
        "  print(f'Average weighted F-score: {np.mean(weighted_f1):.4f}, stdev: {np.std(weighted_f1):.4f}')\r\n",
        "  max_i = accuracy.index(max(accuracy))\r\n",
        "  max_p = preds[max_i]\r\n",
        "  print(f\"Number of sentences predicted neutral: {sum([1 if not 1 in p else 0 for p in max_p])} out of {len(max_p)}\")\r\n",
        "  print(classification_report(eval_labels, max_p, target_names=['trust', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise'], digits=4))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7sgxxKck5nD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05cdbe2a-6e92-4527-9a3e-80d8bf98b231"
      },
      "source": [
        "runs = []\r\n",
        "train_evaluate(runs, t, train_labels, e, eval_labels, num_labels, init_lr, epochs, batch_size_train, 3)\r\n",
        "print(f\"Model: {model_name}, initial learning rate = {init_lr}, input size = {input_size}, batch size = {batch_size_train}, epochs = {epochs}\")\r\n",
        "print_results(runs, eval_labels)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: TurkuNLP/bert-base-finnish-cased-v1, initial learning rate = 3e-05, input size = 64, batch size = 16, epochs = 2\n",
            "Accuracy: 0.3578, 0.3689, 0.3557\n",
            "Weighted F-score: 0.4995, 0.5064, 0.4975\n",
            "Average accuracy: 0.3608, stdev: 0.0058\n",
            "Average weighted F-score: 0.5011, stdev: 0.0038\n",
            "Number of sentences predicted neutral: 271 out of 1445\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       trust     0.6417    0.4918    0.5568       244\n",
            "       anger     0.6298    0.4755    0.5419       347\n",
            "anticipation     0.6667    0.3775    0.4821       249\n",
            "     disgust     0.5714    0.3636    0.4444       220\n",
            "        fear     0.6535    0.3825    0.4826       217\n",
            "         joy     0.7088    0.5811    0.6386       222\n",
            "     sadness     0.6218    0.4512    0.5229       215\n",
            "    surprise     0.4944    0.2431    0.3259       181\n",
            "\n",
            "   micro avg     0.6324    0.4285    0.5109      1895\n",
            "   macro avg     0.6235    0.4208    0.4994      1895\n",
            "weighted avg     0.6275    0.4285    0.5064      1895\n",
            " samples avg     0.5218    0.4712    0.4793      1895\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}